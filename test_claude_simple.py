#!/usr/bin/env python3
"""
Test minimal avec Claude AI - Sans DynamoDB
"""

import sys
import os

# VÃ©rifier la clÃ© API
if not os.environ.get('ANTHROPIC_API_KEY'):
    print("âŒ ERREUR: ANTHROPIC_API_KEY non dÃ©finie!")
    print("\nDÃ©finissez-la avec:")
    print("  export ANTHROPIC_API_KEY=sk-ant-votre-cle")
    sys.exit(1)

print("âœ… ANTHROPIC_API_KEY configurÃ©e")
print("ğŸ§ª Test simple avec Claude AI\n")

# Test direct avec LangChain et Claude
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage

# Initialiser Claude
# Essayer avec le nom de modÃ¨le correct
llm = ChatAnthropic(
    model="claude-3-haiku-20240307",  # Version qui fonctionne avec votre clÃ© API
    temperature=0.3
)

print("="*70)
print("ğŸ¤– TEST DIRECT AVEC CLAUDE AI")
print("="*70 + "\n")

# Test 1: Classification d'intention
print("ğŸ“ Test 1: Classification d'intention\n")

messages_test = [
    "Quels sont mes mÃ©dicaments?",
    "J'ai mal Ã  la tÃªte",
    "Bonjour comment Ã§a va?"
]

for msg in messages_test:
    print(f"ğŸ’¬ Message: {msg}")

    system_prompt = """
Tu es un classificateur d'intention pour un assistant mÃ©dical.

Analyse le message et retourne UNE catÃ©gorie parmi:
- medication: Questions sur mÃ©dicaments
- symptom: SymptÃ´mes de santÃ©
- general: Conversation gÃ©nÃ©rale

RÃ©ponds UNIQUEMENT avec le mot-clÃ©.
"""

    try:
        response = llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=msg)
        ])

        print(f"ğŸ¯ Intent dÃ©tectÃ©: {response.content}")
        print()

    except Exception as e:
        print(f"âŒ Erreur: {str(e)}\n")

print("="*70)

# Test 2: RÃ©ponse conversationnelle
print("\nğŸ“ Test 2: GÃ©nÃ©ration de rÃ©ponse\n")

user_message = "Peux-tu m'expliquer comment tu peux m'aider?"

system_prompt = """
Tu es SmartDoc, un assistant mÃ©dical bienveillant pour personnes Ã¢gÃ©es.

RÃ©ponds de maniÃ¨re:
- Simple et claire
- Chaleureuse
- En franÃ§ais
- Avec des phrases courtes
"""

try:
    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_message)
    ])

    print(f"ğŸ’¬ Question: {user_message}")
    print(f"\nğŸ¤– RÃ©ponse de Claude:\n")
    print("-"*70)
    print(response.content)
    print("-"*70)

except Exception as e:
    print(f"âŒ Erreur: {str(e)}")

print("\n" + "="*70)
print("âœ… Tests terminÃ©s!")
print("\nğŸ“Š RÃ©sultat:")
print("  â€¢ Claude AI fonctionne correctement")
print("  â€¢ Les rÃ©ponses sont naturelles et contextuelles")
print("  â€¢ PrÃªt pour les tests LangGraph!")
print("\nğŸš€ Prochaine Ã©tape:")
print("  â†’ Tester avec LangGraph: python test_with_claude.py")
print("="*70)
